{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PM2.5台灣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load_data\"></a>\n",
    "## 上傳台灣2015一整年空氣監測資料 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟1: 將2015空氣監控資料，上傳至bluemix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_hadoop_config(credentials):\n",
    "    prefix = \"fs.swift.service.\" + credentials['name'] \n",
    "    hconf = sc._jsc.hadoopConfiguration()\n",
    "    hconf.set(prefix + \".auth.url\", credentials['auth_url']+'/v3/auth/tokens')\n",
    "    hconf.set(prefix + \".auth.endpoint.prefix\", \"endpoints\")\n",
    "    hconf.set(prefix + \".tenant\", credentials['project_id'])\n",
    "    hconf.set(prefix + \".username\", credentials['user_id'])\n",
    "    hconf.set(prefix + \".password\", credentials['password'])\n",
    "    hconf.setInt(prefix + \".http.port\", 8080)\n",
    "    hconf.set(prefix + \".region\", credentials['region'])\n",
    "    hconf.setBoolean(prefix + \".public\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟2: 將上傳資料與目前機器建立連結 `Insert to code` (畫面的右邊)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credentials_3 = {\n",
    "  'auth_url':'https://identity.open.softlayer.com',\n",
    "  'project':'object_storage_46719cd2_5b49_4783_8b86_f8a11f0314d4',\n",
    "  'project_id':'646b1215221e4e7d8254de7bdf43bf1f',\n",
    "  'region':'dallas',\n",
    "  'user_id':'e0e7b9afb0884dfc8639dbf8606db94c',\n",
    "  'domain_id':'a299c7413da54b60925ac070303b298e',\n",
    "  'domain_name':'1159491',\n",
    "  'username':'admin_ac4deff35b52ba280c7b60966e456e345cd3f69c',\n",
    "  'password':\"\"\"jbE.e.rL^P9Z.9MC\"\"\",\n",
    "  'filename':'pm2.5Taiwan.csv',\n",
    "  'container':'notebooks',\n",
    "  'tenantId':'sabe-89ac5efdbd68bd-a6554b527697'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟3: 將我們存放於open stack的資料集取名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credentials_3['name'] = 'pm25'\n",
    "set_hadoop_config(credentials_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can access the data by using the preconfigured `SparkContext` function in your notebook. \n",
    "\n",
    "First create a resilient distributed dataset ([`RDD`](https://spark.apache.org/docs/1.6.0/programming-guide.html#resilient-distributed-datasets-rdds)) of the raw data file. You can refer to it by using a URI of the form:\n",
    "\n",
    "```\n",
    "swift://container_name.name/object_name\n",
    "```\n",
    "\n",
    "If you didn't define a container name, at the time you created the Object Storage instance, the default container name that is used is `notebooks`. In this case, you can run the next code cell as it stands. Otherwise, change the container name. For `name` insert the Hadoop configuration name you defined, for example, `keystone`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather = sc.textFile(\"swift://notebooks.pm25/pm2.5Taiwan.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟4: 試試看是否成功上傳  (使用count( ), first( ), collect( ), take( ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 練習1: 讓我們求取2015年，大里每小時的平均pm25數值。\n",
    "## 注意事項：\n",
    "1. 資料分割：原始資料每一行為一個觀測值，我們必須將資料進行分割，才能逐一計算與進行操作。\n",
    "2. 資料清洗：在氣象局的原始資料裡，有些數值由於當初偵測時有異常，所以會加註特別符號如\\*\\#等特殊符號，這些數值我們必須先經過前處理，我們才能進行算術運算。\n",
    "3. 資料選擇：將大里資料挑選出來\n",
    "4. 產生key-value，也就是(小時,pm25值)\n",
    "5. 利用flatMap(), reduceByKey(), groupByKey()，將不同日期但相同時間的pm25值收集起來。\n",
    "6. 計算平均值, 標準差, 最大最小值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟一：資料分割 (使用map () 與 split( ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟二：將大里站資料從全部資料集中挑選出來 (filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "須留意unicode與string的差別,  u'大里'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟三：資料清洗 (使用 map(), str.strip())\n",
    "#### 在氣象局的原始資料裡，有些數值由於當初偵測時有異常，所以會加註特別符號如\\*\\#等特殊符號，或者沒有取到數值為一空值，這些數值我們必須先經過前處理，我們才能進行算術運算。\n",
    "1. ```2015/01/29 大里 PM2.5 14 14 12 7 1 0 4 6 12 16 15  52x 38x 30 29 27 29 24 24 21 19 19 23```\n",
    "2. ```2015/01/16 大里 PM2.5 16 15 17 16 16 13 5 10 14 30 30 25 -4# 22 23 30 33 40 43 45 37 34 38 43``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟四：key value pair的產生 (重要的操作概念)\n",
    "*將每小時資料轉成(小時,pm數值)，以求取每小時的平均值。\n",
    "\n",
    "例如：\n",
    "    2015/01/01 大里 PM2.5 53 55 58 53 43 36 35 42 55 64 65 59 52 44 47 41 43 40 42 35 28 20 18 16\n",
    "    --> [(3, 53) (4, 55) (5, 58) (6, 53) (7, 43) (8, 36) (9, 35) (10, 42) (11, 55) (12, 64) (13, 65) (14, 59) (15, 52) (16, 44) (17, 47) (18, 41) (19, 43) (20, 40) (21, 42) (22, 35) (23, 28) (24, 20) (25, 18) (26, 16)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟五： 利用flatMap(), reduceByKey(), groupByKey()，將不同日期但相同時間的pm25值收集起來。(使用flatMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟六： 計算大里區每個小時區間中，平均之pm25數值 (使用reduceByKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟七： 根據pm25平均濃度，進行排序。使用top( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步驟八： 計算每個時間點的統計值，例如最大值、最小值、平均值、標準差(使用 groupByKey()與mapValues())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 練習2: 請求取2015年，全國pm2.5最高的前十個工作站測點以及其日期。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 練習3: 請算算看2015全國哪個測站，紫爆天數最多？\n",
    "### 假設當日平均值大於60，則算該日該地區紫爆\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "cds_ax_spark.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}